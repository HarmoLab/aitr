{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "private_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK6wS4KG4H9i"
      },
      "source": [
        "# ラベルに関する設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IWnDSQJtYA6"
      },
      "source": [
        "# 画像を格納したzipファイルの名前\n",
        "class_names = [\"beef\", \"pork\"]\n",
        "\n",
        "# class_names = [\"class1\", \"class2\"]\n",
        "# この場合，以下のファイルが Google Drive にアップロードされている想定\n",
        "#   class1.zip\n",
        "#   class2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WY8xwTIu3mA"
      },
      "source": [
        "# GPU割当・ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHC9SdO9goYx"
      },
      "source": [
        "# GPU の確認\n",
        "!nvidia-smi\n",
        "# Google Drive のマウント（Google Drive のファイルを読み書きできるようにする）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ATRP-NpC8v"
      },
      "source": [
        "# 事前に upload したファイルを Google Drive からコピー\n",
        "\n",
        "data_path = \"/content/drive/My Drive/Colaboratory/jts_private/\"\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "to_data_dir = \"data/\"\n",
        "os.makedirs(to_data_dir, exist_ok=True)\n",
        "\n",
        "zip_fname_list = [cn + \".zip\" for cn in class_names]\n",
        "for fname in zip_fname_list:\n",
        "    shutil.copy(data_path + fname, to_data_dir + fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWxlDGQ36aWy"
      },
      "source": [
        "# zipファイルの解凍\n",
        "import zipfile\n",
        "for zip_name in zip_fname_list:\n",
        "    with zipfile.ZipFile(to_data_dir + zip_name) as zf:\n",
        "        zf.extractall(to_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5CzuXoLnnlT"
      },
      "source": [
        "# 交差検証用のデータ分割，ラベル付"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-7_J7rBoWHL"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAZdJ9VDpZHO"
      },
      "source": [
        "def split_train_test(class_kind, fold):\n",
        "    paths = []\n",
        "    labels = []\n",
        "    for idx, kind in enumerate(class_kind):\n",
        "        p = Path(\"./data/{}/\".format(kind))\n",
        "        imgs = list(p.glob(\"*\"))\n",
        "        for imgpath in imgs:\n",
        "            paths.append(imgpath)\n",
        "            labels.append(idx)\n",
        "\n",
        "    paths = np.array(paths)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # 分割処理\n",
        "    kf = KFold(n_splits=fold, random_state=0, shuffle=True)\n",
        "\n",
        "    for idx, (train_index, test_index) in enumerate(kf.split(paths)):\n",
        "\n",
        "        # 訓練データ\n",
        "        with open(\"./data/train_kfold_{:02}.csv\".format(idx), \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"filename\", \"label\"])\n",
        "            for path, label in zip(paths[train_index], labels[train_index]):\n",
        "                writer.writerow([path, label])\n",
        "\n",
        "        # テストデータ\n",
        "        with open(\"./data/test_kfold_{:02}.csv\".format(idx), \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"filename\", \"label\"])\n",
        "            for path, label in zip(paths[test_index], labels[test_index]):\n",
        "                writer.writerow([path, label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu4jeFhppupa"
      },
      "source": [
        "fold_num = 5\n",
        "split_train_test(class_names, fold_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJp-8UAvyDt"
      },
      "source": [
        "# 学習コードの実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E98Gl9BWAHeT"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T11M0mSAPdj"
      },
      "source": [
        "# 各種設定\n",
        "BATCH_SIZE = 50\n",
        "MAX_EPOCH = 25\n",
        "IMAGE_SIZE = 224\n",
        "# シード値の固定\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# GPU利用設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device =\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1jw1oZIAT5L"
      },
      "source": [
        "# 自作のデータセットの処理\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None, loader=default_loader):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir  # 画像が入っているディレクトリのパス\n",
        "        self.loader = loader\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # dataloaderで読み込むときの処理\n",
        "    def __getitem__(self, idx):\n",
        "        # filename取得\n",
        "        img_name = self.df.iloc[idx, 0]\n",
        "        # 画像のパス設定\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        # 画像読み込み\n",
        "        image = self.loader(img_path)\n",
        "        # user_id = self.df.iloc[idx, 2]\n",
        "\n",
        "        label = self.df.iloc[idx, 1]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label, img_path\n",
        "\n",
        "\n",
        "# データの前処理を定義\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        # 画像のリサイズ\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        # tensorに変換\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 訓練用のデータ前処理を定義\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        # 画像のリサイズ\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        # ここにData augmentation処理を記述する\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.9, contrast=0.9),\n",
        "        # tensorに変換\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5WIXFfmAczs"
      },
      "source": [
        "K = 5\n",
        "train_dataset_list = []\n",
        "test_dataset_list = []\n",
        "train_loader_list = []\n",
        "valid_loader_list = []\n",
        "test_loader_list = []\n",
        "model_list = []\n",
        "\n",
        "for i in range(K):\n",
        "    # 訓練データを取得\n",
        "    train_dataset = MyDataset(\n",
        "        csv_file=\"./data/train_kfold_{:02}.csv\".format(i),\n",
        "        root_dir='./',  # 画像を保存したディレクトリ(適宜書き換えて)\n",
        "        transform=transform,\n",
        "    )\n",
        "    # 訓練データの一部を検証データとして使用\n",
        "    num_train = len(train_dataset)\n",
        "    num_valid = int(num_train * 0.2)\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
        "        train_dataset,\n",
        "        [num_train - num_valid, num_valid],\n",
        "    )\n",
        "    # DataLoaderを作成\n",
        "\n",
        "    # 訓練時のtransformを設定\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    # テストデータも同様\n",
        "    test_dataset = MyDataset(\n",
        "        \"./data/test_kfold_{:02}.csv\".format(i),\n",
        "        root_dir='./',\n",
        "        transform=transform,\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # ニューラルネットワーク\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    # print(model)\n",
        "    model.fc = nn.Linear(512, 2)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_dataset_list.append(train_dataset)\n",
        "    test_dataset_list.append(test_dataset)\n",
        "    valid_loader_list.append(valid_loader)\n",
        "    train_loader_list.append(train_loader)\n",
        "    test_loader_list.append(test_loader)\n",
        "    model_list.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Ek_B8uA6y4"
      },
      "source": [
        "v_acc_list = []\n",
        "t_acc_list = []\n",
        "v_accuracy_list = []\n",
        "cm_list = []\n",
        "test_accuracy_list = []\n",
        "for i in range(K):\n",
        "    # 損失関数\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # 勾配降下法を行うoptimizer\n",
        "    optimizer = optim.Adam(model_list[i].parameters(), lr=0.0001)\n",
        "\n",
        "    start = time.time()\n",
        "    print('processing train data ...')\n",
        "\n",
        "    v_acc = []\n",
        "    t_acc = []\n",
        "    best_v_acc = 0\n",
        "    v_accuracy = []\n",
        "    print(\"epoch\\ttrain loss\\tvalid loss\\tvalid accuracy\\tprocessed time\")\n",
        "    for epoch in range(MAX_EPOCH):\n",
        "        model_list[i].train()\n",
        "        train_loss_list = []\n",
        "        # DataLoaderをfor文で回すと入力と正解ラベルが得られる\n",
        "        for x, label, img_path in train_loader_list[i]:\n",
        "            x = x.to(device)\n",
        "            label = label.to(device)\n",
        "            # 勾配を0に初期化\n",
        "            optimizer.zero_grad()\n",
        "            # 順伝播\n",
        "            output = model_list[i](x)\n",
        "            # 誤差の計算\n",
        "            loss = loss_function(output, label)\n",
        "            # 誤差逆伝播\n",
        "            loss.backward()\n",
        "            # パラメータ更新\n",
        "            optimizer.step()\n",
        "            # ミニバッチの訓練誤差をlistに追加\n",
        "            train_loss_list.append(loss.item())\n",
        "        # 各ミニバッチでの訓練誤差の平均を取り，本エポックでの訓練誤差とする\n",
        "        train_loss_mean = np.mean(train_loss_list)\n",
        "\n",
        "        # 検証データでも同様に誤差を計算\n",
        "        # モデルを評価する時は model.eval() とする\n",
        "        model_list[0].eval()\n",
        "        valid_loss_list = []\n",
        "        valid_correct, valid_total = 0, 0\n",
        "        for x, label, img_path in valid_loader_list[i]:\n",
        "            x = x.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model_list[i](x)\n",
        "            loss = loss_function(output, label)\n",
        "            valid_loss_list.append(loss.item())\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            # 正解ラベルと比較，一致している数を加算\n",
        "            valid_correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "            # 正解率(accuracy)を出すためにテストデータの数も加算\n",
        "            valid_total += label.size()[0]\n",
        "\n",
        "        valid_loss_mean = np.mean(valid_loss_list)\n",
        "\n",
        "        valid_accuracy = valid_correct / valid_total\n",
        "\n",
        "        print(\"{0}\\t{1:.6f}\\t{2:.6f}\\t{3:.6f}\\t{4:.6f}\".format(epoch, train_loss_mean, valid_loss_mean, valid_accuracy, time.time() - start))\n",
        "        t_acc.append(train_loss_mean)\n",
        "        v_acc.append(valid_loss_mean)\n",
        "        v_accuracy.append(valid_accuracy)\n",
        "        # モデル保存\n",
        "        if valid_accuracy > best_v_acc:\n",
        "            best_v_acc = valid_accuracy\n",
        "            model_dir = \"./model/\"\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "            save_path = model_dir + \"cnn{:02}.pt\".format(i)\n",
        "            torch.save(model_list[i].state_dict(), save_path)\n",
        "\n",
        "    # 可視化コード\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.plot(t_acc, label=\"train loss\", marker=\"o\")\n",
        "    ax.plot(v_acc, label=\"valid loss\", marker=\"*\")\n",
        "    ax.legend()\n",
        "    plt.savefig(\"losscurve.png\", bbox_inches=\"tight\")\n",
        "\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.plot(v_accuracy, label=\"valid acc\", marker=\"o\")\n",
        "    ax.legend()\n",
        "    plt.savefig(\"acccurve.png\", bbox_inches=\"tight\")\n",
        "\n",
        "    # モデル読込\n",
        "    model_list[i].load_state_dict(torch.load(\"./model/cnn{:02}.pt\".format(i)))\n",
        "\n",
        "    # モデルの評価(テストデータを使用)\n",
        "    print('processing test data ...')\n",
        "    model_list[i].eval()\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    for n, (x, label, img_path) in enumerate(test_loader_list[i]):\n",
        "        x = x.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model_list[i](x)\n",
        "        loss = loss_function(output, label)\n",
        "        # 出力値が最大のインデックスを取得\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "        preds = pred if n == 0 else torch.cat([preds, pred])\n",
        "        labels = label if n == 0 else torch.cat([labels, label])\n",
        "\n",
        "        # 正解ラベルと比較，一致している数を加算\n",
        "        test_correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "        # 正解率(accuracy)を出すためにテストデータの数も加算\n",
        "        test_total += label.size()[0]\n",
        "\n",
        "    test_accuracy = test_correct / test_total\n",
        "    print(\"Test accuracy :\", test_accuracy)\n",
        "\n",
        "    cm = confusion_matrix(labels.cpu(), preds.cpu())\n",
        "\n",
        "    v_acc_list.append(v_acc)\n",
        "    t_acc_list.append(t_acc)\n",
        "    v_accuracy_list.append(v_accuracy)\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    cm_list.append(cm)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oOT6wj1wCn6"
      },
      "source": [
        "# **分析コードを書きましょう**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU2J3QzfwM3S"
      },
      "source": [
        "## テストデータに対するAccuracyの平均値を算出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7B0pIoXhSIc"
      },
      "source": [
        "np.mean(test_accuracy_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoWQY_EWwYGt"
      },
      "source": [
        "## 各データ分割におけるテストデータの混同行列を表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1QZklNhBh1r"
      },
      "source": [
        "# 混同行列 をきれいに出力するためのコード\n",
        "def tune_str(x, class_names=class_names):\n",
        "    max_length = max([len(cn) for cn in class_names])\n",
        "    x = str(x)\n",
        "    while len(x) < max_length:\n",
        "        x = \" \" + x\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZnjwSo7h8ES"
      },
      "source": [
        "for i, cm in enumerate(cm_list):\n",
        "    print(\"分割:\", i)\n",
        "    header = \"{} \".format(tune_str(\" \"))\n",
        "    for class_name in class_names:\n",
        "        header += \" {} \".format(tune_str(class_name))\n",
        "    print(header)\n",
        "    for row_i, class_name_i in enumerate(class_names):\n",
        "        txt = \"{} \".format(tune_str(class_name_i))\n",
        "        for col_i, class_name_j in enumerate(class_names):\n",
        "            txt += \" {} \".format(tune_str(cm[row_i][col_i]))\n",
        "        print(txt)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxOIU4Clyhvh"
      },
      "source": [
        "## 誤認識があった画像パスを出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXVfIyNUX0fD"
      },
      "source": [
        "for i in range(K):\n",
        "    print(\"分割:\", i)\n",
        "    for n, (x, label, img_path) in enumerate(test_loader_list[i]):\n",
        "        x = x.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model_list[i](x)\n",
        "        # 出力値が最大のインデックスを取得\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        \n",
        "        # ここに処理を記述\n",
        "        cnt = 0\n",
        "        for p, l in zip(pred, label):\n",
        "            if p != l:\n",
        "                print(img_path[cnt])\n",
        "            cnt += 1\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}